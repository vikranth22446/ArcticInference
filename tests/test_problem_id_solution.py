#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯æ–°çš„req_idåˆ°problem_idæ˜ å°„è§£å†³æ–¹æ¡ˆæ˜¯å¦æ­£å¸¸å·¥ä½œ

ä½¿ç”¨æ–¹æ³•:
1. ç¡®ä¿åœ¨æ­£ç¡®çš„Dockerç¯å¢ƒä¸­è¿è¡Œ
2. ç¡®ä¿VLLM_USE_V1=1ç¯å¢ƒå˜é‡å·²è®¾ç½®
3. è¿è¡Œ: python test_problem_id_solution.py
"""

import sys
import os

def test_imports():
    """æµ‹è¯•æ‰€æœ‰å¿…è¦çš„å¯¼å…¥"""
    print("=== æµ‹è¯•å¯¼å…¥ ===")
    
    try:
        # æµ‹è¯•ArcticInferenceæ ¸å¿ƒç»„ä»¶å¯¼å…¥
        from arctic_inference.vllm.model_runner import ProblemIdContextManager
        print("âœ“ ProblemIdContextManagerå¯¼å…¥æˆåŠŸ")
        
        from arctic_inference.vllm.llm import apply_llm_patches, LLMPatch
        print("âœ“ LLM patchesç»„ä»¶å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•vLLMå¯¼å…¥
        import vllm
        from vllm.entrypoints.llm import LLM
        print(f"âœ“ vLLMç‰ˆæœ¬: {vllm.__version__}")
        
        return True
    except ImportError as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_llm_patches():
    """æµ‹è¯•LLM patchesçš„åº”ç”¨å’ŒåŠŸèƒ½"""
    print("\n=== æµ‹è¯•LLM Patches ===")
    
    try:
        from arctic_inference.vllm.llm import apply_llm_patches
        from vllm.entrypoints.llm import LLM
        
        # åº”ç”¨patches
        apply_llm_patches()
        
        # æ£€æŸ¥patchesæ˜¯å¦å·²åº”ç”¨
        if hasattr(LLM, '_arctic_problem_id_patched'):
            print("âœ“ LLM patcheså·²æˆåŠŸåº”ç”¨")
            
            # æ£€æŸ¥åŸå§‹æ–¹æ³•æ˜¯å¦è¢«ä¿å­˜
            if hasattr(LLMPatch, '_orig_generate'):
                print("âœ“ åŸå§‹æ–¹æ³•å·²æ­£ç¡®ä¿å­˜")
            else:
                print("âœ— åŸå§‹æ–¹æ³•ä¿å­˜å¤±è´¥")
                return False
            
            # æµ‹è¯•generateæ–¹æ³•ç­¾å
            import inspect
            sig = inspect.signature(LLM.generate)
            if 'problem_ids' in sig.parameters:
                print("âœ“ generateæ–¹æ³•å·²åŒ…å«problem_idså‚æ•°")
            else:
                print("âœ— generateæ–¹æ³•ç¼ºå°‘problem_idså‚æ•°")
                return False
                
            return True
        else:
            print("âœ— LLM patchesæœªæ­£ç¡®åº”ç”¨")
            return False
            
    except Exception as e:
        print(f"âœ— LLM patchesæµ‹è¯•å¼‚å¸¸: {e}")
        return False


def test_context_manager():
    """æµ‹è¯•ProblemIdContextManagerçš„æ–°åŠŸèƒ½"""
    print("\n=== æµ‹è¯•ProblemIdContextManager ===")
    
    try:
        from arctic_inference.vllm.model_runner import ProblemIdContextManager
        
        # æµ‹è¯•req_idæ˜ å°„åŠŸèƒ½
        test_mapping = {"req_1": "problem_1", "req_2": "problem_2", "req_3": "problem_3"}
        
        # è®¾ç½®æ˜ å°„
        ProblemIdContextManager.set_req_id_to_problem_id_mapping(test_mapping)
        
        # éªŒè¯æ˜ å°„
        retrieved_mapping = ProblemIdContextManager.get_req_id_to_problem_id_mapping()
        if retrieved_mapping == test_mapping:
            print("âœ“ req_idæ˜ å°„è®¾ç½®å’Œè·å–æ­£å¸¸")
        else:
            print(f"âœ— req_idæ˜ å°„ä¸åŒ¹é…: æœŸæœ› {test_mapping}, å¾—åˆ° {retrieved_mapping}")
            return False
        
        # æµ‹è¯•å•ä¸ªreq_idæŸ¥æ‰¾
        for req_id, expected_problem_id in test_mapping.items():
            actual_problem_id = ProblemIdContextManager.get_problem_id_for_req_id(req_id)
            if actual_problem_id == expected_problem_id:
                print(f"âœ“ req_idæŸ¥æ‰¾ {req_id}: {actual_problem_id}")
            else:
                print(f"âœ— req_idæŸ¥æ‰¾ {req_id}: æœŸæœ› {expected_problem_id}, å¾—åˆ° {actual_problem_id}")
                return False
        
        # æµ‹è¯•æ¸…ç†åŠŸèƒ½
        ProblemIdContextManager.clear_req_id_mapping()
        after_clear = ProblemIdContextManager.get_req_id_to_problem_id_mapping()
        if len(after_clear) == 0:
            print("âœ“ æ˜ å°„æ¸…ç†æˆåŠŸ")
        else:
            print(f"âœ— æ˜ å°„æ¸…ç†å¤±è´¥ï¼Œä»æœ‰æ•°æ®: {after_clear}")
            return False
        
        return True
    except Exception as e:
        print(f"âœ— ProblemIdContextManageræµ‹è¯•å¼‚å¸¸: {e}")
        return False


def test_integration():
    """é›†æˆæµ‹è¯•ï¼šæ¨¡æ‹Ÿå®Œæ•´çš„å·¥ä½œæµç¨‹"""
    print("\n=== é›†æˆæµ‹è¯• ===")
    
    try:
        from arctic_inference.vllm.model_runner import ProblemIdContextManager
        from arctic_inference.vllm.llm import apply_llm_patches
        
        # ç¡®ä¿patcheså·²åº”ç”¨
        apply_llm_patches()
        
        # æ¨¡æ‹Ÿvllm_rollout_spmd.pyçš„è°ƒç”¨æµç¨‹
        problem_ids = ["problem_001", "problem_002", "problem_003"]
        
        # 1. åˆå§‹åŒ–ä¸Šä¸‹æ–‡ï¼ˆæ¨¡æ‹Ÿvllm_rollout_spmd.pyä¸­çš„ä»£ç ï¼‰
        ProblemIdContextManager.clear_req_id_mapping()
        ProblemIdContextManager.set_req_id_to_problem_id_mapping({})
        print("âœ“ ä¸Šä¸‹æ–‡åˆå§‹åŒ–å®Œæˆ")
        
        # 2. æ¨¡æ‹ŸLLM.generateè°ƒç”¨ï¼ˆå®é™…æƒ…å†µä¸‹ä¼šé€šè¿‡patcheså»ºç«‹æ˜ å°„ï¼‰
        # è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨å»ºç«‹æ˜ å°„æ¥æ¨¡æ‹Ÿpatchesçš„æ•ˆæœ
        simulated_mapping = {
            "0": "problem_001",
            "1": "problem_002", 
            "2": "problem_003"
        }
        ProblemIdContextManager.set_req_id_to_problem_id_mapping(simulated_mapping)
        print("âœ“ æ¨¡æ‹Ÿæ˜ å°„å»ºç«‹å®Œæˆ")
        
        # 3. æ¨¡æ‹Ÿexecute_modelä¸­çš„æŸ¥æ‰¾ï¼ˆé€šè¿‡GPUModelRunnerPatchï¼‰
        for req_id, expected_problem_id in simulated_mapping.items():
            # è¿™æ¨¡æ‹Ÿäº†execute_modelä¸­çš„æŸ¥æ‰¾è¿‡ç¨‹
            retrieved_problem_id = ProblemIdContextManager.get_problem_id_for_req_id(req_id)
            if retrieved_problem_id == expected_problem_id:
                print(f"âœ“ execute_modelæ¨¡æ‹ŸæŸ¥æ‰¾ req_id={req_id} -> problem_id={retrieved_problem_id}")
            else:
                print(f"âœ— execute_modelæ¨¡æ‹ŸæŸ¥æ‰¾å¤±è´¥ req_id={req_id}: æœŸæœ› {expected_problem_id}, å¾—åˆ° {retrieved_problem_id}")
                return False
        
        print("âœ“ é›†æˆæµ‹è¯•å®Œæˆ - å®Œæ•´å·¥ä½œæµç¨‹æ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"âœ— é›†æˆæµ‹è¯•å¼‚å¸¸: {e}")
        return False


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡å’Œé…ç½®"""
    print("=== æ£€æŸ¥ç¯å¢ƒ ===")
    
    # æ£€æŸ¥VLLM_USE_V1
    if os.getenv("VLLM_USE_V1") == "1":
        print("âœ“ VLLM_USE_V1=1 å·²è®¾ç½®")
    else:
        print("âš  VLLM_USE_V1 æœªè®¾ç½®ä¸º1ï¼Œæ’ä»¶å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    
    # æ£€æŸ¥CUDA
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ“ CUDAå¯ç”¨ï¼Œè®¾å¤‡æ•°: {torch.cuda.device_count()}")
        else:
            print("âš  CUDAä¸å¯ç”¨")
    except ImportError:
        print("âš  PyTorchæœªå®‰è£…")
    
    print(f"âœ“ Pythonç‰ˆæœ¬: {sys.version}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("req_idåˆ°problem_idæ˜ å°„è§£å†³æ–¹æ¡ˆæµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    check_environment()
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    tests = [
        ("å¯¼å…¥æµ‹è¯•", test_imports),
        ("LLM Patchesæµ‹è¯•", test_llm_patches),
        ("ä¸Šä¸‹æ–‡ç®¡ç†å™¨æµ‹è¯•", test_context_manager),
        ("é›†æˆæµ‹è¯•", test_integration),
    ]
    
    results = {}
    for test_name, test_func in tests:
        print(f"\n{'='*20}")
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"âœ— {test_name}å¼‚å¸¸: {e}")
            results[test_name] = False
    
    # æ€»ç»“ç»“æœ
    print(f"\n{'='*20}")
    print("æµ‹è¯•ç»“æœæ€»ç»“:")
    all_passed = True
    for test_name, passed in results.items():
        status = "âœ“ é€šè¿‡" if passed else "âœ— å¤±è´¥"
        print(f"  {test_name}: {status}")
        if not passed:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼è§£å†³æ–¹æ¡ˆå·¥ä½œæ­£å¸¸ã€‚")
        print("\nğŸ“ ä¸‹ä¸€æ­¥ï¼šåœ¨å®é™…ç¯å¢ƒä¸­æµ‹è¯•vLLMRolloutä¸problem_idså‚æ•°çš„å®Œæ•´å·¥ä½œæµç¨‹")
        return 0
    else:
        print("\nâŒ æŸäº›æµ‹è¯•å¤±è´¥ã€‚è¯·æ£€æŸ¥é…ç½®å’Œä»£ç ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())