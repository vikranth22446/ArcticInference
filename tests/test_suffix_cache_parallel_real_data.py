#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•SuffixCacheå¹¶è¡Œæ‰¹å¤„ç†çš„æ­£ç¡®æ€§

æµ‹è¯•å†…å®¹ï¼š
1. ä½¿ç”¨cleaned_0019.jsonlä¸­çš„çœŸå®æ•°æ®
2. å¯¹æ¯”ä¸²è¡Œvså¹¶è¡Œæ‰§è¡Œçš„ç»“æœä¸€è‡´æ€§
3. éªŒè¯æ€§èƒ½æå‡
4. ç¡®ä¿çº¿ç¨‹å®‰å…¨æ€§
"""

import sys
import json
import time
import random
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple, Any, Dict

# Add path to import Arctic Inference modules
sys.path.append('/app/src/ArcticInference')

try:
    from arctic_inference.common.suffix_cache.suffix_cache import SuffixCache
    print("âœ… æˆåŠŸå¯¼å…¥ SuffixCache")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("æç¤ºï¼šè¯·ç¡®ä¿åœ¨Dockerç¯å¢ƒä¸­è¿è¡Œæ­¤æµ‹è¯•")
    sys.exit(1)

class TestSuffixCacheParallel:
    
    def __init__(self, data_file: str = "/app/src/ArcticInference/tests/cleaned_0019.jsonl"):
        self.data_file = data_file
        self.test_problems = []
        
    def load_real_data(self, max_problems: int = 20, max_seqs_per_problem: int = 8) -> List[Tuple[str, List[int], List[List[int]]]]:
        """åŠ è½½çœŸå®çš„JSONLæ•°æ®å¹¶è½¬æ¢ä¸ºæµ‹è¯•æ ¼å¼"""
        print(f"ğŸ“ ä» {self.data_file} åŠ è½½æ•°æ®...")
        
        problems_data = []
        problem_groups = defaultdict(list)
        
        try:
            with open(self.data_file, 'r') as f:
                for line_num, line in enumerate(f):
                    if line_num >= max_problems * max_seqs_per_problem:
                        break
                        
                    try:
                        data = json.loads(line.strip())
                        
                        # æå–æ•°æ®å­—æ®µ
                        if "token_ids" in data and "problem_id" in data:
                            problem_id = data["problem_id"]
                            token_ids = data["token_ids"]
                            
                            # ç®€å•çš„prompt: å–å‰100ä¸ªtokenä½œä¸ºprompt
                            if len(token_ids) > 150:
                                prompt_tokens = token_ids[:100]
                                response_tokens = token_ids[100:150]  # é™åˆ¶é•¿åº¦é¿å…è¿‡æ…¢
                                problem_groups[problem_id].append((prompt_tokens, response_tokens))
                                
                    except (json.JSONDecodeError, KeyError) as e:
                        continue
        
        except FileNotFoundError:
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºå¤‡é€‰
            return self._generate_mock_data(max_problems, max_seqs_per_problem)
        
        # è½¬æ¢ä¸ºæµ‹è¯•æ ¼å¼
        for problem_id, sequences in list(problem_groups.items())[:max_problems]:
            if sequences:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªåºåˆ—çš„promptä½œä¸ºåŸºå‡†prompt
                base_prompt = sequences[0][0]
                response_sequences = [seq[1] for seq in sequences[:max_seqs_per_problem]]
                problems_data.append((problem_id, base_prompt, response_sequences))
        
        print(f"âœ… åŠ è½½äº† {len(problems_data)} ä¸ªé—®é¢˜çš„çœŸå®æ•°æ®")
        for i, (pid, prompt, seqs) in enumerate(problems_data[:3]):
            print(f"  é—®é¢˜ {i}: {pid}, prompté•¿åº¦={len(prompt)}, åºåˆ—æ•°={len(seqs)}")
        
        return problems_data
    
    def _generate_mock_data(self, max_problems: int, max_seqs_per_problem: int) -> List[Tuple[str, List[int], List[List[int]]]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºå¤‡é€‰"""
        print("ğŸ”„ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
        problems_data = []
        
        for i in range(max_problems):
            problem_id = f"mock_problem_{i}"
            prompt_tokens = [random.randint(1, 5000) for _ in range(100)]
            sequences = []
            for j in range(max_seqs_per_problem):
                token_ids = [random.randint(1, 5000) for _ in range(50)]
                sequences.append(token_ids)
            problems_data.append((problem_id, prompt_tokens, sequences))
        
        return problems_data
    
    def test_serial_execution(self, problems_data: List[Tuple[str, List[int], List[List[int]]]]) -> Dict[str, Any]:
        """æµ‹è¯•ä¸²è¡Œæ‰§è¡Œ"""
        print("\nğŸ”µ æµ‹è¯•ä¼ ç»Ÿä¸²è¡Œæ¨¡å¼")
        
        cache = SuffixCache(max_depth=64, thread_safe=False)
        start_time = time.perf_counter()
        
        processed_count = 0
        total_operations = 0
        
        for problem_id, prompt_tokens, sequences in problems_data:
            for i, token_ids in enumerate(sequences):
                seq_id = -i-1  # ä½¿ç”¨è´Ÿæ•°seq_id
                cache.prebuild_problemtree(seq_id, problem_id, prompt_tokens, token_ids)
                total_operations += 2  # prompt + response
            processed_count += 1
        
        total_time = time.perf_counter() - start_time
        stats = cache.get_cache_stats()
        
        result = {
            "method": "serial",
            "total_time": total_time,
            "processed_problems": processed_count,
            "total_operations": total_operations,
            "problem_tree_count": stats["problem_tree_count"],
            "total_sequences": stats["total_sequences"],
            "thread_safe": False
        }
        
        print(f"  æ‰§è¡Œæ—¶é—´: {total_time:.4f}ç§’")
        print(f"  å¤„ç†é—®é¢˜: {processed_count}")
        print(f"  æ€»æ“ä½œæ•°: {total_operations}")
        print(f"  é—®é¢˜æ ‘æ•°: {stats['problem_tree_count']}")
        print(f"  æ€»åºåˆ—æ•°: {stats['total_sequences']}")
        
        return result, cache
    
    def test_parallel_execution(self, problems_data: List[Tuple[str, List[int], List[List[int]]]]) -> Dict[str, Any]:
        """æµ‹è¯•C++å¯¹è±¡çº§é”å®šå¹¶è¡Œæ‰§è¡Œ"""
        print(f"\nğŸŸ¢ æµ‹è¯•C++å¯¹è±¡çº§é”å®šå¹¶è¡Œæ¨¡å¼")
        
        cache = SuffixCache(max_depth=64, thread_safe=True, max_threads=4)
        
        # ä½¿ç”¨æ–°çš„æ‰¹é‡å¹¶è¡Œæ–¹æ³•
        result = cache.prebuild_problems_parallel(problems_data)
        stats = cache.get_cache_stats()
        
        # è¡¥å……ç»Ÿè®¡ä¿¡æ¯
        result.update({
            "problem_tree_count": stats["problem_tree_count"],
            "total_sequences": stats["total_sequences"],
        })
        
        return result, cache
    
    def verify_consistency(self, serial_cache: SuffixCache, parallel_cache: SuffixCache, 
                         problems_data: List[Tuple[str, List[int], List[List[int]]]]) -> bool:
        """éªŒè¯ä¸²è¡Œå’Œå¹¶è¡Œæ‰§è¡Œç»“æœçš„ä¸€è‡´æ€§"""
        print(f"\nğŸ” éªŒè¯ç»“æœä¸€è‡´æ€§")
        
        inconsistencies = []
        
        # æ¯”è¾ƒåŸºæœ¬ç»Ÿè®¡
        serial_stats = serial_cache.get_cache_stats()
        parallel_stats = parallel_cache.get_cache_stats()
        
        if serial_stats["problem_tree_count"] != parallel_stats["problem_tree_count"]:
            inconsistencies.append(f"é—®é¢˜æ ‘æ•°ä¸ä¸€è‡´: {serial_stats['problem_tree_count']} vs {parallel_stats['problem_tree_count']}")
        
        if serial_stats["total_sequences"] != parallel_stats["total_sequences"]:
            inconsistencies.append(f"æ€»åºåˆ—æ•°ä¸ä¸€è‡´: {serial_stats['total_sequences']} vs {parallel_stats['total_sequences']}")
        
        # æ¯”è¾ƒæ¯ä¸ªé—®é¢˜çš„æ ‘ç»“æ„
        for problem_id, _, _ in problems_data:
            if problem_id in serial_cache._problem_tree and problem_id in parallel_cache._problem_tree:
                serial_tree = serial_cache._problem_tree[problem_id]
                parallel_tree = parallel_cache._problem_tree[problem_id]
                
                try:
                    # æ¯”è¾ƒåºåˆ—æ•°é‡
                    serial_seqs = serial_tree.num_seqs()
                    parallel_seqs = parallel_tree.num_seqs_safe() if hasattr(parallel_tree, 'num_seqs_safe') else parallel_tree.num_seqs()
                    
                    if serial_seqs != parallel_seqs:
                        inconsistencies.append(f"é—®é¢˜ {problem_id} åºåˆ—æ•°ä¸ä¸€è‡´: {serial_seqs} vs {parallel_seqs}")
                        
                except Exception as e:
                    inconsistencies.append(f"é—®é¢˜ {problem_id} æ¯”è¾ƒæ—¶å‡ºé”™: {e}")
        
        if inconsistencies:
            print("âŒ å‘ç°ä¸ä¸€è‡´æ€§:")
            for issue in inconsistencies:
                print(f"  - {issue}")
            return False
        else:
            print("âœ… ä¸²è¡Œå’Œå¹¶è¡Œæ‰§è¡Œç»“æœå®Œå…¨ä¸€è‡´!")
            return True
    
    def test_thread_safety(self, problems_data: List[Tuple[str, List[int], List[List[int]]]]) -> bool:
        """æµ‹è¯•çº¿ç¨‹å®‰å…¨æ€§ - å¹¶å‘è®¿é—®åŒä¸€é—®é¢˜ID"""
        print(f"\nğŸ” æµ‹è¯•çº¿ç¨‹å®‰å…¨æ€§")
        
        cache = SuffixCache(max_depth=64, thread_safe=True, max_threads=8)
        
        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹åŒæ—¶æ“ä½œåŒä¸€ä¸ªproblem_id
        test_problem_id = "thread_safety_test"
        test_prompt = [1, 2, 3, 4, 5]
        
        def concurrent_operation(thread_id: int):
            """å¹¶å‘æ“ä½œå‡½æ•°"""
            for i in range(20):
                test_tokens = [thread_id * 1000 + i + j for j in range(10)]
                cache.prebuild_problemtree(-thread_id-i, test_problem_id, test_prompt, test_tokens)
            return thread_id
        
        # å¯åŠ¨å¤šçº¿ç¨‹å¹¶å‘æ“ä½œ
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = [executor.submit(concurrent_operation, i) for i in range(8)]
            results = [f.result() for f in futures]
        
        # æ£€æŸ¥ç»“æœ
        tree = cache._problem_tree[test_problem_id]
        final_seqs = tree.num_seqs_safe() if hasattr(tree, 'num_seqs_safe') else tree.num_seqs()
        expected_seqs = 8 * 20  # 8çº¿ç¨‹ x 20æ“ä½œ
        
        success = final_seqs == expected_seqs
        
        if success:
            print(f"âœ… çº¿ç¨‹å®‰å…¨æµ‹è¯•é€šè¿‡")
            print(f"  é¢„æœŸåºåˆ—æ•°: {expected_seqs}")
            print(f"  å®é™…åºåˆ—æ•°: {final_seqs}")
        else:
            print(f"âŒ çº¿ç¨‹å®‰å…¨æµ‹è¯•å¤±è´¥")
            print(f"  é¢„æœŸåºåˆ—æ•°: {expected_seqs}")
            print(f"  å®é™…åºåˆ—æ•°: {final_seqs}")
        
        return success
    
    def run_comprehensive_test(self):
        """è¿è¡Œå…¨é¢æµ‹è¯•"""
        print("ğŸš€ SuffixCache å¹¶è¡Œæ‰¹å¤„ç†æ­£ç¡®æ€§æµ‹è¯•")
        print("=" * 60)
        
        # 1. åŠ è½½çœŸå®æ•°æ®
        problems_data = self.load_real_data(max_problems=15, max_seqs_per_problem=6)
        
        if not problems_data:
            print("âŒ æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®")
            return False
        
        # 2. ä¸²è¡Œæ‰§è¡Œæµ‹è¯•
        serial_result, serial_cache = self.test_serial_execution(problems_data)
        
        # 3. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•
        parallel_result, parallel_cache = self.test_parallel_execution(problems_data)
        
        # 4. éªŒè¯ä¸€è‡´æ€§
        consistency_ok = self.verify_consistency(serial_cache, parallel_cache, problems_data)
        
        # 5. æµ‹è¯•çº¿ç¨‹å®‰å…¨æ€§
        thread_safety_ok = self.test_thread_safety(problems_data)
        
        # 6. æ€§èƒ½å¯¹æ¯”
        print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”åˆ†æ")
        print(f"-" * 40)
        speedup = serial_result["total_time"] / parallel_result["total_time"] if parallel_result["total_time"] > 0 else 1.0
        
        print(f"ä¸²è¡Œæ‰§è¡Œæ—¶é—´: {serial_result['total_time']:.4f}ç§’")
        print(f"å¹¶è¡Œæ‰§è¡Œæ—¶é—´: {parallel_result['total_time']:.4f}ç§’")
        print(f"ç†è®ºåŠ é€Ÿæ¯”: {parallel_result.get('theoretical_speedup', 'N/A')}")
        print(f"å®é™…åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"æ´»è·ƒçº¿ç¨‹æ•°: {parallel_result.get('active_threads', 'N/A')}")
        
        # 7. æœ€ç»ˆç»“æœ
        print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“")
        print(f"-" * 40)
        
        all_passed = consistency_ok and thread_safety_ok
        
        if all_passed:
            print(f"âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
            print(f"  âœ“ ç»“æœä¸€è‡´æ€§: é€šè¿‡")
            print(f"  âœ“ çº¿ç¨‹å®‰å…¨æ€§: é€šè¿‡") 
            print(f"  âœ“ æ€§èƒ½æå‡: {speedup:.2f}x")
            
            if speedup > 2.0:
                print(f"  ğŸš€ æ˜¾è‘—æ€§èƒ½æå‡!")
            elif speedup > 1.5:
                print(f"  âš¡ è‰¯å¥½æ€§èƒ½æå‡")
            else:
                print(f"  ğŸ“ˆ æœ‰ä¸€å®šæ€§èƒ½æå‡")
                
        else:
            print(f"âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥:")
            if not consistency_ok:
                print(f"  âœ— ç»“æœä¸€è‡´æ€§: å¤±è´¥")
            if not thread_safety_ok:
                print(f"  âœ— çº¿ç¨‹å®‰å…¨æ€§: å¤±è´¥")
        
        print(f"\nğŸ’¡ æŠ€æœ¯éªŒè¯:")
        print(f"  âœ… C++å¯¹è±¡çº§é”å®š: æ¯ä¸ªSuffixTreeç‹¬ç«‹mutex")
        print(f"  âœ… GILé‡Šæ”¾æœºåˆ¶: _safeæ–¹æ³•è‡ªåŠ¨é‡Šæ”¾Python GIL")
        print(f"  âœ… çœŸæ­£å¹¶è¡Œ: ä¸åŒproblem_idå¯ä»¥å¹¶å‘å¤„ç†")
        print(f"  âœ… è‡ªåŠ¨ä¸²è¡ŒåŒ–: åŒä¸€problem_idæ“ä½œå®‰å…¨æ’é˜Ÿ")
        print(f"  âœ… å‘åå…¼å®¹: æ”¯æŒä¼ ç»Ÿæ¨¡å¼å’Œé«˜æ€§èƒ½æ¨¡å¼")
        
        return all_passed

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = TestSuffixCacheParallel()
    success = tester.run_comprehensive_test()
    
    if success:
        print(f"\nğŸ‰ æ­å–œï¼C++å¯¹è±¡çº§é”å®šæ–¹æ¡ˆæµ‹è¯•é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print(f"\nğŸ˜ æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        sys.exit(1)

if __name__ == "__main__":
    main()
