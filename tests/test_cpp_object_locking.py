#!/usr/bin/env python3
# Copyright 2025 Snowflake Inc.
# SPDX-License-Identifier: Apache-2.0

"""
æµ‹è¯•C++å¯¹è±¡çº§é”å®šæ–¹æ¡ˆ

éªŒè¯ï¼š
1. çº¿ç¨‹å®‰å…¨æ–¹æ³•æ˜¯å¦å­˜åœ¨
2. GILæ˜¯å¦çœŸæ­£è¢«é‡Šæ”¾
3. ä¸åŒproblem_idæ˜¯å¦èƒ½çœŸæ­£å¹¶è¡Œ
4. åŒä¸€problem_idæ˜¯å¦ä¿æŒä¸²è¡Œ
5. æ€§èƒ½æå‡æ˜¯å¦æ˜¾è‘—
"""

import sys
import os
import time
import random
import threading
import multiprocessing
from typing import List, Tuple
from concurrent.futures import ThreadPoolExecutor

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/data/zshao/rllm/ArcticInference')

def test_thread_safe_methods_exist():
    """æµ‹è¯•çº¿ç¨‹å®‰å…¨æ–¹æ³•æ˜¯å¦å­˜åœ¨"""
    print("ğŸ” æµ‹è¯•1: æ£€æŸ¥çº¿ç¨‹å®‰å…¨æ–¹æ³•æ˜¯å¦å­˜åœ¨")
    
    try:
        from arctic_inference.common.suffix_cache._C import SuffixTree
        
        # åˆ›å»ºæµ‹è¯•æ ‘
        tree = SuffixTree(64)
        
        # æ£€æŸ¥çº¿ç¨‹å®‰å…¨æ–¹æ³•
        safe_methods = ['append_safe', 'extend_safe', 'num_seqs_safe']
        missing_methods = []
        
        for method in safe_methods:
            if hasattr(tree, method):
                print(f"  âœ… {method} æ–¹æ³•å­˜åœ¨")
            else:
                missing_methods.append(method)
                print(f"  âŒ {method} æ–¹æ³•ç¼ºå¤±")
        
        if missing_methods:
            print(f"  âš ï¸ ç¼ºå¤±æ–¹æ³•: {missing_methods}")
            print(f"  è¯·ç¡®ä¿å·²é‡æ–°ç¼–è¯‘C++ä»£ç ï¼špip install -e .[vllm] -v")
            return False
        else:
            print(f"  âœ… æ‰€æœ‰çº¿ç¨‹å®‰å…¨æ–¹æ³•éƒ½å­˜åœ¨")
            return True
            
    except ImportError as e:
        print(f"  âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_gil_release_effectiveness():
    """æµ‹è¯•GILé‡Šæ”¾çš„æœ‰æ•ˆæ€§"""
    print(f"\nâš¡ æµ‹è¯•2: éªŒè¯GILé‡Šæ”¾æ•ˆæœ")
    
    try:
        from arctic_inference.common.suffix_cache._C import SuffixTree
        
        def cpu_intensive_task_safe(tree, seq_id_start, num_ops=1000):
            """ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•çš„CPUå¯†é›†å‹ä»»åŠ¡"""
            for i in range(num_ops):
                seq_id = seq_id_start + i
                tokens = [random.randint(1, 1000) for _ in range(50)]
                tree.extend_safe(seq_id, tokens)
            return num_ops
        
        def cpu_intensive_task_unsafe(tree, seq_id_start, num_ops=1000):
            """ä½¿ç”¨éçº¿ç¨‹å®‰å…¨æ–¹æ³•çš„CPUå¯†é›†å‹ä»»åŠ¡"""
            for i in range(num_ops):
                seq_id = seq_id_start + i
                tokens = [random.randint(1, 1000) for _ in range(50)]
                tree.extend(seq_id, tokens)
            return num_ops
        
        # åˆ›å»ºå¤šä¸ªç‹¬ç«‹çš„æ ‘ï¼ˆé¿å…å¯¹è±¡é”å†²çªï¼‰
        trees_safe = [SuffixTree(64) for _ in range(4)]
        trees_unsafe = [SuffixTree(64) for _ in range(4)]
        
        # æµ‹è¯•çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬
        print("  æµ‹è¯•çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼ˆåº”è¯¥æœ‰å¹¶è¡Œæ•ˆæœï¼‰...")
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [
                executor.submit(cpu_intensive_task_safe, trees_safe[i], i * 1000, 500) 
                for i in range(4)
            ]
            results_safe = [f.result() for f in futures]
        
        safe_time = time.perf_counter() - start_time
        
        # æµ‹è¯•éçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬
        print("  æµ‹è¯•éçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼ˆåº”è¯¥ä¸²è¡ŒåŒ–ï¼‰...")
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [
                executor.submit(cpu_intensive_task_unsafe, trees_unsafe[i], i * 1000, 500)
                for i in range(4)
            ]
            results_unsafe = [f.result() for f in futures]
        
        unsafe_time = time.perf_counter() - start_time
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = unsafe_time / safe_time if safe_time > 0 else 1.0
        
        print(f"  çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬æ—¶é—´: {safe_time:.4f}ç§’")
        print(f"  éçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬æ—¶é—´: {unsafe_time:.4f}ç§’") 
        print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        if speedup > 1.5:
            print(f"  âœ… GILé‡Šæ”¾ç”Ÿæ•ˆï¼çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬æœ‰æ˜æ˜¾åŠ é€Ÿ")
            return True
        elif speedup > 1.1:
            print(f"  âš ï¸ GILé‡Šæ”¾éƒ¨åˆ†ç”Ÿæ•ˆï¼Œæœ‰ä¸€å®šåŠ é€Ÿ")
            return True
        else:
            print(f"  âŒ GILé‡Šæ”¾å¯èƒ½æœªç”Ÿæ•ˆï¼Œæ— æ˜æ˜¾åŠ é€Ÿ")
            return False
            
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_per_object_locking():
    """æµ‹è¯•per-objecté”å®šæœºåˆ¶"""
    print(f"\nğŸ”’ æµ‹è¯•3: éªŒè¯per-objecté”å®šæœºåˆ¶")
    
    try:
        from arctic_inference.common.suffix_cache._C import SuffixTree
        
        shared_tree = SuffixTree(64)
        results = []
        lock = threading.Lock()
        
        def worker_on_same_tree(worker_id, num_ops=500):
            """å¤šä¸ªçº¿ç¨‹æ“ä½œåŒä¸€ä¸ªæ ‘ï¼ˆåº”è¯¥ä¸²è¡ŒåŒ–ï¼‰"""
            ops_completed = 0
            for i in range(num_ops):
                seq_id = worker_id * 1000 + i
                tokens = [worker_id, i] * 10  # å¯åŒºåˆ†çš„tokenåºåˆ—
                shared_tree.extend_safe(seq_id, tokens)
                ops_completed += 1
            
            with lock:
                results.append((worker_id, ops_completed))
            return ops_completed
        
        # å¤šä¸ªçº¿ç¨‹æ“ä½œåŒä¸€ä¸ªæ ‘
        print("  å¤šä¸ªçº¿ç¨‹æ“ä½œåŒä¸€ä¸ªSuffixTree...")
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [
                executor.submit(worker_on_same_tree, worker_id, 300)
                for worker_id in range(4)
            ]
            thread_results = [f.result() for f in futures]
        
        same_tree_time = time.perf_counter() - start_time
        
        # éªŒè¯ç»“æœæ­£ç¡®æ€§
        total_seqs = shared_tree.num_seqs_safe()
        expected_seqs = sum(thread_results)
        
        print(f"  æ‰§è¡Œæ—¶é—´: {same_tree_time:.4f}ç§’")
        print(f"  æœŸæœ›åºåˆ—æ•°: {expected_seqs}")
        print(f"  å®é™…åºåˆ—æ•°: {total_seqs}")
        
        if total_seqs == expected_seqs:
            print(f"  âœ… Per-objecté”å®šå·¥ä½œæ­£å¸¸ï¼Œæ•°æ®ä¸€è‡´æ€§ä¿æŒ")
            return True
        else:
            print(f"  âŒ æ•°æ®ä¸ä¸€è‡´ï¼Œper-objecté”å®šå¯èƒ½æœ‰é—®é¢˜")
            return False
            
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print(f"\nğŸ“Š æµ‹è¯•4: å®Œæ•´æ€§èƒ½å¯¹æ¯”")
    
    try:
        from arctic_inference.common.suffix_cache.thread_safe_suffix_cache import ThreadSafeSuffixCache
        from arctic_inference.common.suffix_cache.suffix_cache import SuffixCache
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        def generate_test_data(num_problems=16, seqs_per_problem=8, tokens_per_seq=1000):
            problems_data = []
            for problem_id in range(num_problems):
                prompt_tokens = [random.randint(1, 5000) for _ in range(200)]
                sequences = []
                for seq_id in range(seqs_per_problem):
                    token_ids = [random.randint(1, 5000) for _ in range(tokens_per_seq)]
                    sequences.append(token_ids)
                problems_data.append((f"problem_{problem_id}", prompt_tokens, sequences))
            return problems_data
        
        problems_data = generate_test_data(20, 6, 1500)
        total_operations = len(problems_data) * 6 * 2  # problems * seqs * (extend prompt + extend tokens)
        
        print(f"  æµ‹è¯•é…ç½®: {len(problems_data)} problems, æ€»æ“ä½œæ•°: {total_operations}")
        
        # æµ‹è¯•ä¼ ç»Ÿä¸²è¡Œæ–¹æ³•
        print("  æµ‹è¯•ä¼ ç»ŸSuffixCacheï¼ˆä¸²è¡Œï¼‰...")
        traditional_cache = SuffixCache(max_depth=64)
        
        start_time = time.perf_counter()
        for problem_id, prompt_tokens, sequences in problems_data:
            for i, token_ids in enumerate(sequences):
                traditional_cache.prebuild_problemtree(-i-1, problem_id, prompt_tokens, token_ids)
        serial_time = time.perf_counter() - start_time
        
        # æµ‹è¯•çº¿ç¨‹å®‰å…¨å¹¶è¡Œæ–¹æ³•
        print("  æµ‹è¯•ThreadSafeSuffixCacheï¼ˆå¹¶è¡Œï¼‰...")
        thread_safe_cache = ThreadSafeSuffixCache(max_depth=64, max_threads=8)
        
        result = thread_safe_cache.build_problems_parallel(problems_data)
        parallel_time = result['total_time']
        
        # æ€§èƒ½å¯¹æ¯”
        speedup = serial_time / parallel_time if parallel_time > 0 else 1.0
        
        print(f"\n  ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
        print(f"    ä¸²è¡Œæ—¶é—´: {serial_time:.4f}ç§’")
        print(f"    å¹¶è¡Œæ—¶é—´: {parallel_time:.4f}ç§’")
        print(f"    åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"    ç†è®ºåŠ é€Ÿ: {result.get('theoretical_speedup', 1.0):.2f}x")
        print(f"    æˆåŠŸå¤„ç†: {result.get('successful_problems', 0)} problems")
        
        if speedup > 2.0:
            print(f"    âœ… æ˜¾è‘—åŠ é€Ÿï¼C++å¯¹è±¡çº§é”å®šæ–¹æ¡ˆæˆåŠŸ")
            return True
        elif speedup > 1.3:
            print(f"    âš¡ æœ‰ä¸€å®šåŠ é€Ÿæ•ˆæœ")
            return True
        else:
            print(f"    âš ï¸ åŠ é€Ÿæ•ˆæœæœ‰é™")
            return False
            
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ C++å¯¹è±¡çº§é”å®šæ–¹æ¡ˆéªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    test_results = []
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_results.append(("çº¿ç¨‹å®‰å…¨æ–¹æ³•æ£€æŸ¥", test_thread_safe_methods_exist()))
    
    if test_results[-1][1]:  # å¦‚æœæ–¹æ³•å­˜åœ¨æ‰ç»§ç»­æµ‹è¯•
        test_results.append(("GILé‡Šæ”¾æ•ˆæœéªŒè¯", test_gil_release_effectiveness()))
        test_results.append(("Per-objecté”å®šéªŒè¯", test_per_object_locking()))
        test_results.append(("å®Œæ•´æ€§èƒ½å¯¹æ¯”", test_performance_comparison()))
    
    # æ±‡æ€»ç»“æœ
    print(f"\n" + "=" * 60)
    print(f"ğŸ¯ æµ‹è¯•ç»“æœæ±‡æ€»:")
    
    passed_tests = 0
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
        if result:
            passed_tests += 1
    
    print(f"\næ€»è®¡: {passed_tests}/{len(test_results)} æµ‹è¯•é€šè¿‡")
    
    if passed_tests == len(test_results):
        print(f"ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼C++å¯¹è±¡çº§é”å®šæ–¹æ¡ˆå·¥ä½œå®Œç¾")
        print(f"âœ… å¯ä»¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ThreadSafeSuffixCache")
    elif passed_tests >= len(test_results) - 1:
        print(f"âš¡ å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œæ–¹æ¡ˆåŸºæœ¬å¯ç”¨")  
        print(f"âš ï¸ è¯·æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•é¡¹")
    else:
        print(f"âŒ å¤šé¡¹æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
        print(f"ğŸ’¡ è¯·ç¡®ä¿ï¼š")
        print(f"   1. é‡æ–°ç¼–è¯‘äº†C++ä»£ç ")
        print(f"   2. çº¿ç¨‹å®‰å…¨æ–¹æ³•æ­£ç¡®å®ç°")
        print(f"   3. Pythonç»‘å®šæ­£ç¡®é…ç½®")
    
    return passed_tests == len(test_results)


if __name__ == "__main__":
    success = main()
