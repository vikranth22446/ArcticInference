#!/usr/bin/env python3
"""
æµ‹è¯•model_runner.pyä¸­çš„å‡½æ•°ä¸çº¿ç¨‹å®‰å…¨SuffixCacheçš„é›†æˆ

éªŒè¯ï¼š
1. _update_suffix_cacheå‡½æ•°ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„cache_promptå’Œupdate_response
2. propose_suffix_draft_token_idså‡½æ•°ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„speculate
3. å¹¶å‘è°ƒç”¨è¿™äº›å‡½æ•°æ—¶çš„çº¿ç¨‹å®‰å…¨æ€§
"""

import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from typing import List

sys.path.append('/app/src/ArcticInference')

try:
    from arctic_inference.common.suffix_cache.suffix_cache import SuffixCache, SuffixSpecResult
    
    print("ğŸ§ª model_runner.pyå‡½æ•°ä¸çº¿ç¨‹å®‰å…¨SuffixCacheé›†æˆæµ‹è¯•")
    print("=" * 60)
    
    def test_basic_thread_safety():
        """æµ‹è¯•åŸºæœ¬çš„çº¿ç¨‹å®‰å…¨åŠŸèƒ½"""
        print("\nğŸ”§ åŸºæœ¬çº¿ç¨‹å®‰å…¨åŠŸèƒ½æµ‹è¯•")
        
        # çº¿ç¨‹å®‰å…¨æ¨¡å¼
        cache = SuffixCache(max_depth=64, thread_safe=True, max_threads=4)
        
        # æ¨¡æ‹Ÿmodel_runner.pyä¸­_update_suffix_cacheçš„è°ƒç”¨
        def simulate_update_suffix_cache(req_id: str, problem_id: str, prompt_tokens: List[int], sampled_tokens: List[int]):
            """æ¨¡æ‹Ÿ_update_suffix_cacheå‡½æ•°çš„è¡Œä¸º"""
            # 1. æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜prompt
            if not cache.has_cached_prompt(req_id):
                # 2. ç¼“å­˜prompt (å¯¹åº”model_runner.pyçš„cache_promptè°ƒç”¨)
                cache.cache_prompt(req_id, prompt_tokens)
            
            # 3. æ›´æ–°response (å¯¹åº”model_runner.pyçš„update_responseè°ƒç”¨)
            cache.update_response(req_id, problem_id, sampled_tokens)
        
        # æ¨¡æ‹Ÿmodel_runner.pyä¸­propose_suffix_draft_token_idsçš„è°ƒç”¨
        def simulate_propose_suffix_draft_tokens(req_id: str, problem_id: str, pattern: List[int]) -> SuffixSpecResult:
            """æ¨¡æ‹Ÿpropose_suffix_draft_token_idså‡½æ•°çš„è¡Œä¸º"""
            # å¯¹åº”model_runner.pyçš„speculateè°ƒç”¨
            return cache.speculate(
                req_id=req_id,
                problem_id=problem_id,
                pattern=pattern,
                max_spec_tokens=8,
                max_spec_factor=1.5,
                max_spec_offset=-1,
                min_token_prob=0.1,
                use_cached_prompt=True
            )
        
        # å•çº¿ç¨‹æµ‹è¯•
        test_req_id = "test_req_1" 
        test_problem_id = "test_problem_1"
        test_prompt = [1, 2, 3, 4, 5]
        test_sampled = [6, 7, 8]
        test_pattern = [3, 4, 5, 6, 7, 8]
        
        # æ‰§è¡Œæ¨¡æ‹Ÿçš„model_runnerè°ƒç”¨
        simulate_update_suffix_cache(test_req_id, test_problem_id, test_prompt, test_sampled)
        result = simulate_propose_suffix_draft_tokens(test_req_id, test_problem_id, test_pattern)
        
        print(f"  âœ… å•çº¿ç¨‹æµ‹è¯•æˆåŠŸ")
        print(f"    ç¼“å­˜prompt: {cache.has_cached_prompt(test_req_id)}")
        print(f"    æ¨æµ‹ç»“æœå¾—åˆ†: {result.score:.2f}")
        print(f"    æ¨æµ‹tokenæ•°: {len(result.token_ids)}")
        
        return cache
    
    def test_concurrent_model_runner_calls():
        """æµ‹è¯•å¹¶å‘çš„model_runnerå‡½æ•°è°ƒç”¨"""
        print(f"\nâš¡ å¹¶å‘model_runnerå‡½æ•°è°ƒç”¨æµ‹è¯•")
        
        cache = SuffixCache(max_depth=64, thread_safe=True, max_threads=8)
        
        def concurrent_worker(worker_id: int):
            """å¹¶å‘å·¥ä½œå‡½æ•°ï¼Œæ¨¡æ‹Ÿå¤šä¸ªè¯·æ±‚å¹¶å‘å¤„ç†"""
            results = []
            
            for i in range(10):  # æ¯ä¸ªworkerå¤„ç†10ä¸ªè¯·æ±‚
                req_id = f"worker_{worker_id}_req_{i}"
                problem_id = f"worker_{worker_id}_problem_{i % 3}"  # 3ä¸ªä¸åŒé—®é¢˜
                
                prompt_tokens = [worker_id * 100 + j for j in range(1, 6)]
                sampled_tokens = [worker_id * 100 + 50 + j for j in range(3)]
                pattern = prompt_tokens[-3:] + sampled_tokens
                
                # æ¨¡æ‹Ÿ_update_suffix_cache
                if not cache.has_cached_prompt(req_id):
                    cache.cache_prompt(req_id, prompt_tokens)
                cache.update_response(req_id, problem_id, sampled_tokens)
                
                # æ¨¡æ‹Ÿpropose_suffix_draft_token_ids
                spec_result = cache.speculate(
                    req_id=req_id,
                    problem_id=problem_id, 
                    pattern=pattern,
                    max_spec_tokens=5,
                    max_spec_factor=1.0,
                    min_token_prob=0.1,
                    use_cached_prompt=True
                )
                
                results.append({
                    'req_id': req_id,
                    'problem_id': problem_id,
                    'spec_score': spec_result.score,
                    'spec_tokens': len(spec_result.token_ids)
                })
                
            return results
        
        # å¹¶å‘æ‰§è¡Œ
        start_time = time.perf_counter()
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = [executor.submit(concurrent_worker, i) for i in range(8)]
            all_results = []
            for future in futures:
                all_results.extend(future.result())
        
        total_time = time.perf_counter() - start_time
        
        # ç»Ÿè®¡ç»“æœ
        total_requests = len(all_results)
        unique_req_ids = len(set(r['req_id'] for r in all_results))
        unique_problem_ids = len(set(r['problem_id'] for r in all_results))
        avg_spec_score = sum(r['spec_score'] for r in all_results) / total_requests
        
        cache_stats = cache.get_cache_stats()
        
        print(f"  âœ… å¹¶å‘æµ‹è¯•å®Œæˆ")
        print(f"    å¤„ç†æ—¶é—´: {total_time:.4f}ç§’")
        print(f"    æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"    å”¯ä¸€è¯·æ±‚ID: {unique_req_ids}")
        print(f"    å”¯ä¸€é—®é¢˜ID: {unique_problem_ids}")
        print(f"    å¹³å‡æ¨æµ‹å¾—åˆ†: {avg_spec_score:.3f}")
        print(f"    é—®é¢˜æ ‘æ•°: {cache_stats['problem_tree_count']}")
        print(f"    æç¤ºæ ‘æ•°: {cache_stats['prompt_tree_count']}")
        print(f"    æ€»åºåˆ—æ•°: {cache_stats['total_sequences']}")
        
        return len(all_results) == 80 and unique_req_ids == 80  # 8 workers Ã— 10 requests
    
    def test_thread_safety_stress():
        """å‹åŠ›æµ‹è¯•ï¼šå¤šçº¿ç¨‹åŒæ—¶è¯»å†™ç›¸åŒçš„é—®é¢˜ID"""
        print(f"\nğŸ”¥ çº¿ç¨‹å®‰å…¨å‹åŠ›æµ‹è¯•")
        
        cache = SuffixCache(max_depth=64, thread_safe=True, max_threads=4)
        
        # é¢„å…ˆåˆ›å»ºä¸€ä¸ªå…±äº«çš„é—®é¢˜
        shared_problem_id = "shared_problem"
        shared_req_base = "stress_test_req"
        
        def stress_worker(worker_id: int):
            """å‹åŠ›æµ‹è¯•å·¥ä½œå‡½æ•°"""
            operations = 0
            for i in range(20):
                req_id = f"{shared_req_base}_{worker_id}_{i}"
                
                try:
                    # å¹¶å‘ç¼“å­˜ä¸åŒçš„prompt
                    prompt_tokens = [worker_id * 1000 + j for j in range(5)]
                    cache.cache_prompt(req_id, prompt_tokens)
                    
                    # å¹¶å‘æ›´æ–°ç›¸åŒçš„é—®é¢˜ID (è¿™é‡Œä¼šæœ‰çœŸæ­£çš„å¹¶å‘å†™æ“ä½œ)
                    sampled_tokens = [worker_id * 1000 + 100 + j for j in range(3)]
                    cache.update_response(req_id, shared_problem_id, sampled_tokens)
                    
                    # å¹¶å‘è¿›è¡Œæ¨æµ‹
                    pattern = prompt_tokens[-2:] + sampled_tokens
                    cache.speculate(
                        req_id=req_id,
                        problem_id=shared_problem_id,
                        pattern=pattern,
                        max_spec_tokens=3,
                        use_cached_prompt=True
                    )
                    
                    operations += 3
                except Exception as e:
                    print(f"Worker {worker_id} error: {e}")
                    return 0
                    
            return operations
        
        # é«˜å¹¶å‘æ‰§è¡Œ
        start_time = time.perf_counter()
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(stress_worker, i) for i in range(10)]
            results = [f.result() for f in futures]
        
        total_time = time.perf_counter() - start_time
        total_operations = sum(results)
        
        final_stats = cache.get_cache_stats()
        
        print(f"  âœ… å‹åŠ›æµ‹è¯•å®Œæˆ")
        print(f"    æ‰§è¡Œæ—¶é—´: {total_time:.4f}ç§’")
        print(f"    æ€»æ“ä½œæ•°: {total_operations}")
        print(f"    æ“ä½œ/ç§’: {total_operations/total_time:.0f}")
        print(f"    æœ€ç»ˆçŠ¶æ€: {final_stats['problem_tree_count']} é—®é¢˜æ ‘, {final_stats['prompt_tree_count']} æç¤ºæ ‘")
        
        return total_operations > 0
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    print("ğŸš€ å¼€å§‹æµ‹è¯•...")
    
    # 1. åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    cache1 = test_basic_thread_safety()
    
    # 2. å¹¶å‘è°ƒç”¨æµ‹è¯•
    concurrent_success = test_concurrent_model_runner_calls()
    
    # 3. å‹åŠ›æµ‹è¯•
    stress_success = test_thread_safety_stress()
    
    # æœ€ç»ˆç»“æœ
    print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“")
    print(f"-" * 40)
    
    all_passed = concurrent_success and stress_success
    
    if all_passed:
        print(f"âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print(f"  âœ“ åŸºæœ¬çº¿ç¨‹å®‰å…¨åŠŸèƒ½: é€šè¿‡")
        print(f"  âœ“ å¹¶å‘model_runnerè°ƒç”¨: é€šè¿‡")
        print(f"  âœ“ çº¿ç¨‹å®‰å…¨å‹åŠ›æµ‹è¯•: é€šè¿‡")
    else:
        print(f"âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        
    print(f"\nğŸ’¡ é›†æˆéªŒè¯:")
    print(f"  ğŸ”§ cache_prompt: çº¿ç¨‹å®‰å…¨å­—å…¸æ“ä½œ + çº¿ç¨‹å®‰å…¨extend")
    print(f"  ğŸ”„ update_response: çº¿ç¨‹å®‰å…¨å­—å…¸æ“ä½œ + çº¿ç¨‹å®‰å…¨append/extend")
    print(f"  ğŸ” speculate: å®‰å…¨çš„å¹¶å‘è¯»æ“ä½œ")
    print(f"  ğŸš€ C++å¯¹è±¡çº§é”å®š: æ¯ä¸ªSuffixTreeç‹¬ç«‹ä¿æŠ¤")
    print(f"  âš¡ GILé‡Šæ”¾: _safeæ–¹æ³•è‡ªåŠ¨é‡Šæ”¾Python GIL")
    
    print(f"\nğŸŠ model_runner.pyä¸çº¿ç¨‹å®‰å…¨SuffixCacheå®Œç¾é›†æˆï¼")
    
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
