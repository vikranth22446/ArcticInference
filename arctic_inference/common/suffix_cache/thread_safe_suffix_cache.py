# Copyright 2025 Snowflake Inc.
# SPDX-License-Identifier: Apache-2.0

"""
çº¿ç¨‹å®‰å…¨çš„SuffixCacheå®ç°

æ ¸å¿ƒç‰¹æ€§ï¼š
- ä½¿ç”¨C++å¯¹è±¡çº§é”å®š + GILé‡Šæ”¾å®ç°çœŸæ­£çš„per-objectå¹¶è¡Œ
- ä¸åŒproblem_idçš„SuffixTreeå¯ä»¥çœŸæ­£å¹¶è¡Œæ‰§è¡Œ
- åŒä¸€ä¸ªSuffixTreeçš„æ“ä½œä»ç„¶ä¸²è¡Œï¼ˆç”±C++å¯¹è±¡é”ä¿æŠ¤ï¼‰
- è§£å†³äº†ä¼ ç»Ÿå¤šçº¿ç¨‹çš„GILäº‰å¤ºé—®é¢˜
"""

import threading
import time
import hashlib
from typing import Dict, List, Tuple, Hashable, Sequence, Union, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass

from .suffix_cache import SuffixCache, SuffixSpecResult
from arctic_inference.common.suffix_cache._C import SuffixTree


@dataclass
class ThreadSafeBuildTask:
    """çº¿ç¨‹å®‰å…¨æ„å»ºä»»åŠ¡"""
    thread_id: int
    problems_data: List[Tuple[Hashable, List[int], List[List[int]]]]


@dataclass
class ThreadSafeBuildResult:
    """çº¿ç¨‹å®‰å…¨æ„å»ºç»“æœ"""
    thread_id: int
    problems_processed: int
    total_operations: int
    processing_time: float
    success: bool
    error_msg: str = ""


class ThreadSafeSuffixCache:
    """
    çº¿ç¨‹å®‰å…¨çš„SuffixCache
    
    æ ¸å¿ƒæœºåˆ¶ï¼š
    1. ğŸ“¦ æ¯ä¸ªSuffixTreeæœ‰ç‹¬ç«‹çš„C++å¯¹è±¡é” (std::mutex)
    2. ğŸ”“ C++æ–¹æ³•é‡Šæ”¾GIL (py::call_guard<py::gil_scoped_release>)
    3. âš¡ ä¸åŒproblem_idå¯ä»¥çœŸæ­£å¹¶è¡Œæ‰§è¡Œ
    4. ğŸ”’ åŒä¸€problem_idå†…çš„æ“ä½œä¸²è¡ŒåŒ–ï¼ˆå¯¹è±¡é”ä¿æŠ¤ï¼‰
    
    è¿™æ ·å®ç°äº†ï¼š"ä¸åŒproblem_idæœ‰ç‹¬ç«‹GILï¼ŒåŒä¸€treeå…±äº«GIL"çš„éœ€æ±‚
    """
    
    def __init__(self, max_depth: int = 64, max_threads: int = None):
        self.max_depth = max_depth
        self.max_threads = max_threads or min(8, threading.active_count() + 4)
        
        # ä¸»SuffixCacheï¼ˆä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•ï¼‰
        self.main_cache = SuffixCache(max_depth)
        
        print(f"ThreadSafeSuffixCacheåˆå§‹åŒ–: æœ€å¤§{self.max_threads}ä¸ªçº¿ç¨‹")
    
    def _get_thread_for_problem(self, problem_id: Hashable) -> int:
        """å“ˆå¸Œåˆ†åŒºï¼šå°†problem_idåˆ†é…åˆ°å›ºå®šçº¿ç¨‹"""
        problem_hash = hashlib.md5(str(problem_id).encode()).hexdigest()
        return int(problem_hash, 16) % self.max_threads
    
    def _build_problems_in_thread(self, task: ThreadSafeBuildTask) -> ThreadSafeBuildResult:
        """åœ¨çº¿ç¨‹ä¸­æ„å»ºproblemsï¼ˆä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•ï¼‰"""
        start_time = time.perf_counter()
        
        try:
            problems_processed = 0
            total_operations = 0
            
            for problem_id, prompt_tokens, sequences in task.problems_data:
                # ğŸ”‘ å…³é”®æ­¥éª¤ï¼šç¡®ä¿SuffixTreeå­˜åœ¨
                if problem_id not in self.main_cache._problem_tree:
                    # è¿™ä¸ªæ“ä½œéœ€è¦çº¿ç¨‹åŒæ­¥ï¼ˆè®¿é—®å…±äº«å­—å…¸ï¼‰
                    with threading.Lock():  # ä¿æŠ¤å…±äº«å­—å…¸çš„è®¿é—®
                        if problem_id not in self.main_cache._problem_tree:
                            self.main_cache._problem_tree[problem_id] = SuffixTree(self.max_depth)
                
                # âš¡ å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•ï¼ˆé‡Šæ”¾GIL + å¯¹è±¡é”ï¼‰
                tree = self.main_cache._problem_tree[problem_id]
                
                for i, token_ids in enumerate(sequences):
                    seq_id = -i-1
                    
                    # ğŸš€ è¿™äº›æ“ä½œä¼šï¼š
                    # 1. è·å–treeçš„å¯¹è±¡é” (std::lock_guard<std::mutex>)
                    # 2. é‡Šæ”¾Python GIL (py::call_guard<py::gil_scoped_release>)  
                    # 3. æ‰§è¡ŒC++ä»£ç ï¼ˆçœŸæ­£å¹¶è¡Œï¼ï¼‰
                    tree.extend_safe(seq_id, prompt_tokens)
                    tree.extend_safe(seq_id, token_ids)
                    total_operations += 2
                
                problems_processed += 1
            
            processing_time = time.perf_counter() - start_time
            
            return ThreadSafeBuildResult(
                thread_id=task.thread_id,
                problems_processed=problems_processed,
                total_operations=total_operations,
                processing_time=processing_time,
                success=True
            )
            
        except Exception as e:
            processing_time = time.perf_counter() - start_time
            return ThreadSafeBuildResult(
                thread_id=task.thread_id,
                problems_processed=0,
                total_operations=0,
                processing_time=processing_time,
                success=False,
                error_msg=str(e)
            )
    
    def build_problems_parallel(
        self, 
        problems_data: List[Tuple[Hashable, List[int], List[List[int]]]]
    ) -> Dict[str, Any]:
        """å¹¶è¡Œæ„å»ºproblemsï¼ˆçœŸæ­£çš„per-objectå¹¶è¡Œï¼‰"""
        
        if not problems_data:
            return {"total_problems": 0, "total_time": 0.0, "method": "no_data"}
        
        start_time = time.perf_counter()
        
        # æŒ‰çº¿ç¨‹åˆ†åŒºåˆ†ç»„problems
        thread_problems = [[] for _ in range(self.max_threads)]
        
        for problem_id, prompt_tokens, sequences in problems_data:
            thread_id = self._get_thread_for_problem(problem_id)
            thread_problems[thread_id].append((problem_id, prompt_tokens, sequences))
        
        # åˆ›å»ºçº¿ç¨‹ä»»åŠ¡
        thread_tasks = []
        for thread_id, problems in enumerate(thread_problems):
            if problems:
                task = ThreadSafeBuildTask(thread_id, problems)
                thread_tasks.append(task)
        
        thread_sizes = [len(tp) for tp in thread_problems]
        print(f"çº¿ç¨‹åˆ†å¸ƒ: {thread_sizes}")
        print(f"å¯åŠ¨{len(thread_tasks)}ä¸ªçº¿ç¨‹è¿›è¡ŒçœŸæ­£çš„å¹¶è¡Œæ„å»º")
        
        # ğŸš€ å…³é”®ï¼šä½¿ç”¨çº¿ç¨‹æ±  + çº¿ç¨‹å®‰å…¨æ–¹æ³•å®ç°çœŸæ­£å¹¶è¡Œ
        with ThreadPoolExecutor(max_workers=len(thread_tasks)) as executor:
            from tqdm import tqdm
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(self._build_problems_in_thread, task): task 
                for task in thread_tasks
            }
            
            # æ”¶é›†ç»“æœ
            results = []
            for future in tqdm(as_completed(future_to_task), total=len(thread_tasks), 
                              desc="Building with thread-safe C++ objects"):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    task = future_to_task[future]
                    results.append(ThreadSafeBuildResult(
                        thread_id=task.thread_id,
                        problems_processed=0,
                        total_operations=0,
                        processing_time=0.0,
                        success=False,
                        error_msg=str(e)
                    ))
        
        # ç»Ÿè®¡ç»“æœ
        successful_threads = 0
        total_operations = 0
        processing_times = []
        
        for result in results:
            if result.success:
                successful_threads += 1
                total_operations += result.total_operations
                processing_times.append(result.processing_time)
                print(f"çº¿ç¨‹{result.thread_id}: {result.problems_processed} problems, "
                      f"{result.total_operations} ops, {result.processing_time:.4f}ç§’")
            else:
                print(f"çº¿ç¨‹{result.thread_id} å¤±è´¥: {result.error_msg}")
        
        total_time = time.perf_counter() - start_time
        
        # æ€§èƒ½æŒ‡æ ‡
        parallel_time = max(processing_times) if processing_times else 0.0
        theoretical_speedup = sum(processing_times) / parallel_time if parallel_time > 0 else 1.0
        actual_speedup = sum(processing_times) / total_time if total_time > 0 else 1.0
        
        print(f"çº¿ç¨‹å®‰å…¨å¹¶è¡Œå®Œæˆ:")
        print(f"  å¹¶è¡Œæ‰§è¡Œæ—¶é—´: {parallel_time:.4f}ç§’") 
        print(f"  æ€»æ—¶é—´: {total_time:.4f}ç§’")
        print(f"  ç†è®ºåŠ é€Ÿæ¯”: {theoretical_speedup:.2f}x")
        print(f"  å®é™…åŠ é€Ÿæ¯”: {actual_speedup:.2f}x")
        print(f"  âœ… çœŸæ­£å®ç°äº†per-problem-IDçš„ç‹¬ç«‹å¹¶è¡Œæ‰§è¡Œ")
        
        return {
            "method": f"thread_safe_cpp_locking_{self.max_threads}",
            "total_problems": len(problems_data),
            "successful_problems": sum(r.problems_processed for r in results if r.success),
            "successful_threads": successful_threads,
            "total_operations": total_operations,
            "total_time": total_time,
            "parallel_time": parallel_time,
            "theoretical_speedup": theoretical_speedup,
            "actual_speedup": actual_speedup,
            "thread_distribution": thread_sizes,
            "active_threads": len(thread_tasks)
        }
    
    def prebuild_problemtree_safe(
        self, 
        seq_id: int, 
        problem_id: Hashable,
        prompt_token_ids: Sequence[int], 
        token_ids: Sequence[int]
    ):
        """çº¿ç¨‹å®‰å…¨çš„å•ä¸ªprebuildæ–¹æ³•"""
        # ç¡®ä¿SuffixTreeå­˜åœ¨
        if problem_id not in self.main_cache._problem_tree:
            self.main_cache._problem_tree[problem_id] = SuffixTree(self.max_depth)
        
        # ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
        tree = self.main_cache._problem_tree[problem_id]
        tree.extend_safe(seq_id, list(prompt_token_ids))
        tree.extend_safe(seq_id, list(token_ids))
    
    def speculate(
        self,
        req_id: Hashable,
        problem_id: Hashable,
        pattern: Sequence[int],
        **kwargs
    ) -> SuffixSpecResult:
        """æ¨æµ‹æ–¹æ³•ï¼ˆspeculateå·²ç»æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼‰"""
        return self.main_cache.speculate(req_id, problem_id, pattern, **kwargs)
    
    def get_thread_stats(self) -> Dict[str, Any]:
        """è·å–çº¿ç¨‹å®‰å…¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "max_threads": self.max_threads,
            "problem_tree_count": len(self.main_cache._problem_tree),
            "prompt_tree_count": len(self.main_cache._prompt_trees)
        }
        
        # ç»Ÿè®¡å„ä¸ªçº¿ç¨‹çš„problemåˆ†å¸ƒ
        thread_problems = [0] * self.max_threads
        for problem_id in self.main_cache._problem_tree.keys():
            thread_id = self._get_thread_for_problem(problem_id)
            thread_problems[thread_id] += 1
        
        stats["thread_problem_distribution"] = thread_problems
        
        return stats
    
    def clear_all_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.main_cache.clear_all_cache()


# é€‚é…å™¨ï¼šå…¼å®¹åŸSuffixCacheæ¥å£
class ThreadSafeSuffixCacheAdapter:
    """
    é€‚é…å™¨ï¼Œè®©ThreadSafeSuffixCacheå®Œå…¨å…¼å®¹åŸSuffixCacheæ¥å£
    å¯ä»¥ç›´æ¥æ›¿æ¢åŸæ¥çš„SuffixCacheä½¿ç”¨
    """
    
    def __init__(self, max_depth: int = 64, max_threads: int = None):
        self.thread_safe_cache = ThreadSafeSuffixCache(max_depth, max_threads)
        self._max_depth = max_depth
    
    @property
    def max_depth(self) -> int:
        return self._max_depth
    
    @property
    def _problem_tree(self):
        """ç›´æ¥è®¿é—®ä¸»ç¼“å­˜çš„problem_tree"""
        return self.thread_safe_cache.main_cache._problem_tree
    
    @property
    def _prompt_trees(self):
        """ç›´æ¥è®¿é—®ä¸»ç¼“å­˜çš„prompt_trees"""
        return self.thread_safe_cache.main_cache._prompt_trees
    
    def prebuild_problemtree(
        self, 
        seq_id: int, 
        problem_id: Hashable,
        prompt_token_ids: Sequence[int], 
        token_ids: Sequence[int]
    ):
        """å…¼å®¹æ¥å£ï¼šå•ä¸ªprebuild"""
        self.thread_safe_cache.prebuild_problemtree_safe(
            seq_id, problem_id, prompt_token_ids, token_ids
        )
    
    def speculate(
        self,
        req_id: Hashable,
        problem_id: Hashable,
        pattern: Sequence[int],
        **kwargs
    ) -> SuffixSpecResult:
        """å…¼å®¹æ¥å£ï¼šæ¨æµ‹"""
        return self.thread_safe_cache.speculate(req_id, problem_id, pattern, **kwargs)
    
    def clear_all_cache(self):
        """å…¼å®¹æ¥å£ï¼šæ¸…ç©ºç¼“å­˜"""
        self.thread_safe_cache.clear_all_cache()
    
    # æ‰¹é‡æ„å»ºæ¥å£ï¼ˆæ¨èä½¿ç”¨ï¼‰
    def build_problems_parallel(
        self, 
        problems_data: List[Tuple[Hashable, List[int], List[List[int]]]]
    ) -> Dict[str, Any]:
        """æ‰¹é‡å¹¶è¡Œæ„å»ºï¼ˆæ¨èæ¥å£ï¼‰"""
        return self.thread_safe_cache.build_problems_parallel(problems_data)
